import requests
from bs4 import BeautifulSoup
import re
import csv
from urllib.parse import urljoin, urlparse
import time
from itertools import zip_longest
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def search_websites_with_keywords(keywords):
    search_urls = []
    headers = {'User-Agent': 'Mozilla/5.0'}
    for keyword in keywords:
        query = "+".join(keyword.split())
        google_search_url = f"https://www.google.com/search?q={query}"
        try:
            response = requests.get(google_search_url, headers=headers, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if "/url?q=" in href and "webcache" not in href:
                        extracted_url = href.split("/url?q=")[1].split("&")[0]
                        search_urls.append(extracted_url)
            else:
                logging.warning(f"Failed to search for '{keyword}' (Status code: {response.status_code})")
        except Exception as e:
            logging.error(f"Error searching for '{keyword}': {e}")
    return list(set(search_urls))

def extract_contact_info(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, timeout=10, headers=headers)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')

            # Extract emails
            emails = set(re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', soup.text))

            # Extract phone numbers: Only 10-digit numbers and valid patterns
            phone_numbers = set(re.findall(r'\b(?:\+?\d{1,3}[-.\s]?)?(?:\(?\d{2,4}\)?[-.\s]?)?\d{6,10}\b', soup.text))
            valid_phones = {num for num in phone_numbers if len(re.sub(r'\D', '', num)) in [10, 7]}  # Filter valid numbers
            
            # Extract social media links
            social_links = set(link['href'] for link in soup.find_all('a', href=True) if any(domain in link['href'] for domain in ['facebook.com', 'twitter.com', 'linkedin.com', 'instagram.com']))

            # Extract addresses
            address_candidates = soup.find_all(string=re.compile(r'\d{1,5}\s[\w\s,.]+'))
            addresses = {addr.strip() for addr in address_candidates if ',' in addr}  # Filter for valid addresses

            return emails, valid_phones, social_links, addresses
        else:
            logging.warning(f"Failed to access {url} (Status code: {response.status_code})")
    except Exception as e:
        logging.error(f"Error accessing {url}: {e}")
    return set(), set(), set(), "", set()

def crawl_websites(base_urls, max_depth=2):
    crawled_urls = set()
    all_emails = set()
    all_phone_numbers = set()
    all_social_links = set()
    all_addresses = set()
    
    for base_url in base_urls:
        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(base_url, timeout=10, headers=headers)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                links = [urljoin(base_url, a['href']) for a in soup.find_all('a', href=True)]
                links = [link for link in links if urlparse(link).netloc]
                for link in links[:max_depth]:
                    if link not in crawled_urls:
                        crawled_urls.add(link)
                        emails, phone_numbers, social_links, addresses = extract_contact_info(link)
                        all_emails.update(emails)
                        all_phone_numbers.update(phone_numbers)
                        all_social_links.update(social_links)
                        all_addresses.update(addresses)
                        time.sleep(1)  # Throttle requests
            else:
                logging.warning(f"Failed to access {base_url} (Status code: {response.status_code})")
        except Exception as e:
            logging.error(f"Error accessing {base_url}: {e}")
    return all_emails, all_phone_numbers, all_social_links, all_addresses

if __name__ == "__main__":
    keywords = [
         "Nepal Consultancies", "Nepal", "Study Abroad Australia",
        "Study abroad Canada", "Study in Japan", "student visa in Nepal",
        "ECAN", "Orbit Consultancy", "Student Visa from Nepal"
    ]
    websites = search_websites_with_keywords(keywords)
    logging.info(f"Websites to crawl: {websites}")
    emails, phone_numbers, social_links, addresses = crawl_websites(websites)

    # Save data to CSV
    with open("contacts.csv", "w", newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(["Email", "Phone", "Social Media Links" "Address"])
        for data in zip_longest(emails, phone_numbers, social_links, addresses,  fillvalue=""):
            writer.writerow(data)
