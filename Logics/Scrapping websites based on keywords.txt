import requests
from bs4 import BeautifulSoup
import re
import csv
from urllib.parse import urljoin, urlparse
import time

def search_websites_with_keywords(keywords):
    """
    Search for websites related to specified keywords using a search engine.
    Returns a list of URLs.
    """
    search_urls = []
    headers = {'User-Agent': 'Mozilla/5.0'}

    for keyword in keywords:
        query = "+".join(keyword.split())
        google_search_url = f"https://www.google.com/search?q={query}"
        
        try:
            response = requests.get(google_search_url, headers=headers, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if "/url?q=" in href and "webcache" not in href:
                        # Extract actual URL
                        extracted_url = href.split("/url?q=")[1].split("&")[0]
                        search_urls.append(extracted_url)
            else:
                print(f"Failed to search for '{keyword}' (Status code: {response.status_code})")
        except Exception as e:
            print(f"Error searching for '{keyword}': {e}")

    return list(set(search_urls))  # Return unique URLs

def extract_contact_info(url):
    """
    Extract emails, phone numbers, addresses, and social media links from a webpage.
    """
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, timeout=10, headers=headers)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')

            # Extract emails
            emails = set(re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', soup.text))

            # Extract phone numbers
            phone_numbers = set(re.findall(r'\+?\d{1,3}[-.\s]?\(?\d{1,4}\)?[-.\s]?\d{1,4}[-.\s]?\d{1,9}', soup.text))

            # Extract addresses (based on patterns, may require adjustments for accuracy)
            addresses = set()
            for address in soup.find_all(string=re.compile(r'\d{1,5}\s\w+(\s\w+)*,\s\w+')):
                addresses.add(address.strip())

            # Extract social media links
            social_links = set()
            for link in soup.find_all('a', href=True):
                href = link['href']
                if any(domain in href for domain in ['facebook.com', 'twitter.com', 'linkedin.com', 'instagram.com']):
                    social_links.add(href)

            return emails, phone_numbers, addresses, social_links
        else:
            print(f"Failed to access {url} (Status code: {response.status_code})")
    except Exception as e:
        print(f"Error accessing {url}: {e}")
    return set(), set(), set(), set()

def crawl_websites(base_urls, max_depth=2):
    """
    Crawl a list of websites to extract contact information.
    """
    crawled_urls = set()
    all_emails = set()
    all_phone_numbers = set()
    all_addresses = set()
    all_social_links = set()

    for base_url in base_urls:
        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(base_url, timeout=10, headers=headers)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')

                # Extract links from the base page
                links = [urljoin(base_url, a['href']) for a in soup.find_all('a', href=True)]
                links = [link for link in links if urlparse(link).netloc]  # Filter valid URLs

                for link in links[:max_depth]:  # Limit depth to avoid excessive crawling
                    if link not in crawled_urls:
                        crawled_urls.add(link)
                        emails, phone_numbers, addresses, social_links = extract_contact_info(link)
                        all_emails.update(emails)
                        all_phone_numbers.update(phone_numbers)
                        all_addresses.update(addresses)
                        all_social_links.update(social_links)
            else:
                print(f"Failed to access {base_url} (Status code: {response.status_code})")
        except Exception as e:
            print(f"Error accessing {base_url}: {e}")
    return all_emails, all_phone_numbers, all_addresses, all_social_links

if __name__ == "__main__":
    # Keywords for searching websites
    keywords = [
        "consultancies", "Nepal consultancies", "study abroad australia",
        "study abroad canada", "study in japan", "student visa in Nepal",
        "Ecan", "Orbit", "Consultancies to apply student Visa from Nepal"
    ]

    # Fetch websites from keywords
    websites = search_websites_with_keywords(keywords)
    print(f"Websites to crawl: {websites}")

    # Crawl websites for contact information
    emails, phone_numbers, addresses, social_links = crawl_websites(websites)

    # Display results
    print("\nEmails found:")
    for email in emails:
        print(email)

    print("\nPhone Numbers found:")
    for phone in phone_numbers:
        print(phone)

    print("\nAddresses found:")
    for address in addresses:
        print(address)

    print("\nSocial Media Links found:")
    for link in social_links:
        print(link)

    # Write results to a CSV file
    with open("contacts.csv", "w", newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(["Email", "Phone", "Address", "Social Media Links"])
        for email, phone, address, link in zip(emails, phone_numbers, addresses, social_links):
            writer.writerow([email, phone, address, link])
