import requests
from bs4 import BeautifulSoup
import re
import csv
from urllib.parse import urljoin, urlparse
import time
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def search_websites_with_keywords(keywords):
    search_urls = []
    headers = {'User-Agent': 'Mozilla/5.0'}
    for keyword in keywords:
        query = "+".join(keyword.split())
        google_search_url = f"https://www.google.com/search?q={query}"
        try:
            response = requests.get(google_search_url, headers=headers, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if "/url?q=" in href and "webcache" not in href:
                        extracted_url = href.split("/url?q=")[1].split("&")[0]
                        search_urls.append(extracted_url)
            else:
                logging.warning(f"Failed to search for '{keyword}' (Status code: {response.status_code})")
        except Exception as e:
            logging.error(f"Error searching for '{keyword}': {e}")
    return list(set(search_urls))

def extract_contact_info(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, timeout=10, headers=headers)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')

            # Extract emails
            emails = set(re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', soup.text))

            # Extract valid phone numbers
            raw_phone_numbers = set(re.findall(r'\b\d{7,10}\b', soup.text))
            valid_phones = {
                num for num in raw_phone_numbers if (
                    (num.startswith("98") or num.startswith("97")) and len(num) == 10 or 
                    (num.startswith("+977") and len(num) == 13) or
                    (num.startswith("01-") or num.startswith("01")) and len(num) >= 7
                )
            }

            # Extract social media links
            social_links = set(
                link['href'] for link in soup.find_all('a', href=True) 
                if any(domain in link['href'] for domain in ['facebook.com', 'twitter.com', 'linkedin.com', 'instagram.com'])
            )

            return {"emails": emails, "phones": valid_phones, "social_links": social_links}
        else:
            logging.warning(f"Failed to access {url} (Status code: {response.status_code})")
    except Exception as e:
        logging.error(f"Error accessing {url}: {e}")
    return {"emails": set(), "phones": set(), "social_links": set()}

def crawl_websites(base_urls, max_depth=2):
    crawled_urls = set()
    results = []

    for base_url in base_urls:
        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(base_url, timeout=10, headers=headers)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                links = [urljoin(base_url, a['href']) for a in soup.find_all('a', href=True)]
                links = [link for link in links if urlparse(link).netloc]
                for link in links[:max_depth]:
                    if link not in crawled_urls:
                        crawled_urls.add(link)
                        contact_info = extract_contact_info(link)
                        results.append(contact_info)
                        time.sleep(1)  # Throttle requests
            else:
                logging.warning(f"Failed to access {base_url} (Status code: {response.status_code})")
        except Exception as e:
            logging.error(f"Error accessing {base_url}: {e}")
    return results

if __name__ == "__main__":
    keywords = [
        "Nepal Consultancy", "educational consultancy in Nepal",
"study abroad consultancy Nepal",
"best educational consultancy in Nepal",
"educational consultancy in Kathmandu",
"educational consultancy in Pokhara",
"educational consultancy in Butwal",
"educational consultancy in Bharatpur",
"educational consultancy in Birgunj",
"educational consultancy in Jhapa",
"educational consultancy in Biratnagar",
"educational consultancy in Nepalgunj",
"best education consultancy in Nepal for USA",
"best education consultancy in Nepal for Australia",
"consultancy for abroad studies Nepal",
"consultancy for European countries", "Study in Japan" 
         
    ]
    websites = search_websites_with_keywords(keywords)
    logging.info(f"Websites to crawl: {websites}")
    crawled_data = crawl_websites(websites)

    # Save data to CSV
    with open("Consultancies-Info.csv", "w", newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(["Email", "Phone", "Social Media Links"])
        for entry in crawled_data:
            emails = ", ".join(entry["emails"]) if entry["emails"] else "N/A"
            phones = ", ".join(entry["phones"]) if entry["phones"] else "N/A"
            social_links = ", ".join(entry["social_links"]) if entry["social_links"] else "N/A"
            writer.writerow([emails, phones, social_links])

logging.info("Data saved to Consultancies-Info.csv")
